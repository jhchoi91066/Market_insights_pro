# -*- coding: utf-8 -*-
"""
Amazon Market Insights Pro - FastAPI Main Application
Provides web UI and API endpoints for Amazon market analysis.
"""
from typing import List, Tuple
from fastapi import FastAPI, HTTPException, Request, Form
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel, Field
import os
from functools import lru_cache
import asyncio
import traceback
from datetime import datetime

# --- ë¡œê¹… í—¬í¼ í•¨ìˆ˜ ---
def log_to_file(message: str):
    """ë””ë²„ê¹… ë©”ì‹œì§€ë¥¼ íŒŒì¼ì— ê¸°ë¡í•©ë‹ˆë‹¤."""
    with open("debug_log.txt", "a", encoding="utf-8") as f:
        f.write(f"{datetime.now().isoformat()} - {message}\n")
    print(message) # ì½˜ì†”ì—ë„ ì¶œë ¥

# ë¶„ì„ê¸° ëª¨ë“ˆì€ ì‹œì‘ ì‹œ ë¡œë“œí•´ë„ ì•ˆì „í•©ë‹ˆë‹¤.
from core.analyzer_v2 import SQLiteMarketAnalyzer

app = FastAPI(
    title="Amazon Market Insights Pro",
    description="Amazon product market analysis and competition research tool",
    version="1.0.0", # Amazon conversion complete
)

# --- v2 MVP ì„¤ì • ---
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

# ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ëŠ” ë¯¸ë¦¬ ìƒì„±
sqlite_analyzer = SQLiteMarketAnalyzer()

# --- ë™ì‹œì„± ì œì–´ë¥¼ ìœ„í•œ ê¸€ë¡œë²Œ ë½ ---
import asyncio
scraping_lock = asyncio.Lock()  # í•œ ë²ˆì— í•˜ë‚˜ì˜ ìŠ¤í¬ë˜í•‘ë§Œ í—ˆìš©

# -----------------------------------------------------------------------------
# MVP ì›¹ í˜ì´ì§€ ë¼ìš°íŒ…
# -----------------------------------------------------------------------------
@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    return templates.TemplateResponse("index.html", {"request": request, "title": "Amazon Market Insights Pro"})

@app.post("/report", response_class=HTMLResponse)
async def create_report(request: Request, keyword: str = Form(...)):
    log_to_file(f"--- ë³´ê³ ì„œ ìƒì„± ì‹œì‘: '{keyword}' ---")
    scraper = None # finallyì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë¯¸ë¦¬ ì„ ì–¸
    try:
        # --- ì§€ì—° ë¡œë”©(Lazy Loading) ì ìš© ---
        log_to_file("Scraper ëª¨ë“ˆ ë¡œë”© ì‹œì‘")
        from core.scraper import AmazonScraper
        scraper = AmazonScraper()
        log_to_file("Scraper ëª¨ë“ˆ ë¡œë”© ì™„ë£Œ")

        # English keywords don't require encoding handling

        # --- ë¸Œë¼ìš°ì € ì‹¤í–‰ ---
        log_to_file("ë¸Œë¼ìš°ì € ì‹œì‘")
        await scraper.start_browser()
        log_to_file("ë¸Œë¼ìš°ì € ì‹œì‘ ì™„ë£Œ")

        # 1. ê¸°ì¡´ ë°ì´í„° í™•ì¸
        log_to_file("ê¸°ì¡´ ë°ì´í„° í™•ì¸ ì‹œì‘")
        existing_count = 0
        try:
            existing_check = sqlite_analyzer.analyze_category_competition(keyword)
            existing_count = existing_check.get('competitor_count', 0) if existing_check else 0
        except Exception as e:
            log_to_file(f"ê¸°ì¡´ ë°ì´í„° í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        log_to_file(f"ê¸°ì¡´ ë°ì´í„° {existing_count}ê°œ í™•ì¸")

        # 2. í•„ìš”ì‹œ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰ (ë™ì‹œì„± ì œì–´)
        if existing_count < 30:  # ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•´ ê¸°ì¤€ ìƒí–¥
            log_to_file("ìŠ¤í¬ë˜í•‘ í•„ìš” - ì‘ì—… ì‹œì‘")
            
            # ë™ì‹œì„± ì œì–´: í•œ ë²ˆì— í•˜ë‚˜ì˜ ìŠ¤í¬ë˜í•‘ë§Œ í—ˆìš©
            try:
                # ë½ì´ ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸ (ë…¼ë¸”ë¡œí‚¹)
                if scraping_lock.locked():
                    log_to_file(f"â° ëŒ€ê¸° ì¤‘: ë‹¤ë¥¸ ë¶„ì„ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. '{keyword}' ëŒ€ê¸°ì—´ì— ì¶”ê°€")
                
                async with scraping_lock:
                    log_to_file(f"ğŸ”’ ìŠ¤í¬ë˜í•‘ ë½ íšë“: '{keyword}' ì‘ì—… ì‹œì‘")
                    db_result = await scraper.scrape_and_save_to_db(keyword, max_products=100)
                    log_to_file(f"ğŸ”“ ìŠ¤í¬ë˜í•‘ ë½ í•´ì œ: '{keyword}' ì‘ì—… ì™„ë£Œ")
            except Exception as lock_error:
                log_to_file(f"âŒ ë½ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {lock_error}")
                raise lock_error
            
            log_to_file(f"ìŠ¤í¬ë˜í•‘ ì‘ì—… ì™„ë£Œ. ê²°ê³¼: {db_result.get('success')}")
            if not db_result or not db_result.get('success'):
                error_reason = db_result.get('message', 'Unknown scraping error') if db_result else 'Scraper did not respond'
                log_to_file(f"Scraping failed: {error_reason}")
                return templates.TemplateResponse("error.html", {
                    "request": request,
                    "error_message": f"Failed to collect data for '{keyword}' category",
                    "error_reason": error_reason
                })
        else:
            log_to_file("ìŠ¤í¬ë˜í•‘ ë¶ˆí•„ìš” - ê±´ë„ˆëœ€")

        # 3. ë¶„ì„ ì‹¤í–‰
        log_to_file("ë°ì´í„° ë¶„ì„ ì‹œì‘")
        competition_report = sqlite_analyzer.analyze_category_competition(keyword)
        log_to_file("ê²½ìŸ ë¶„ì„ ì™„ë£Œ")
        saturation_report = sqlite_analyzer.calculate_market_saturation(keyword)
        log_to_file("ì‹œì¥ í¬í™”ë„ ë¶„ì„ ì™„ë£Œ")
        report_data = {**competition_report, **saturation_report, 'keyword': keyword}

        # 4. ê²°ê³¼ í˜ì´ì§€ ë Œë”ë§
        log_to_file("ê²°ê³¼ í˜ì´ì§€ ë Œë”ë§")
        return templates.TemplateResponse("report.html", {"request": request, "report": report_data})
    
    except Exception as e:
        error_trace = traceback.format_exc()
        log_to_file(f"!!! ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        log_to_file(error_trace)
        return templates.TemplateResponse("error.html", {
            "request": request,
            "error_message": "An unexpected error occurred while generating the analysis report.",
            "error_reason": str(e)
        })
    finally:
        if scraper:
            log_to_file("ë¸Œë¼ìš°ì € ì¢…ë£Œ ì‹œì‘")
            await scraper.close_browser()
            log_to_file("ë¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ")
        log_to_file(f"--- ë³´ê³ ì„œ ìƒì„± ì¢…ë£Œ: '{keyword}' ---")
