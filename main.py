# -*- coding: utf-8 -*-
"""
ì´ íŒŒì¼ì€ Market Insights Pro í”„ë¡œì íŠ¸ì˜ FastAPI ì„œë²„ ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤.
ì›¹ UI ë Œë”ë§ê³¼ API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ëª¨ë‘ í¬í•¨í•©ë‹ˆë‹¤.
"""
from typing import List, Tuple
from fastapi import FastAPI, HTTPException, Request, Form, WebSocket, Depends
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import os
import asyncio
import json

# ìºì‹œ ë° ë¶„ì„ ëª¨ë“ˆ
from core.cache import get_cache_manager, CacheManager
from core.analyzer_v2 import SQLiteMarketAnalyzer
from core.scraper import AmazonScraper

app = FastAPI(
    title="Amazon Market Insights Pro",
    description="Amazon product market analysis and competition research tool",
    version="1.2.0", # ë²„ì „ ì—…ë°ì´íŠ¸
)

# --- ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° ì„¤ì • ---
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")
sqlite_analyzer = SQLiteMarketAnalyzer()
scraper = AmazonScraper()
scraping_lock = asyncio.Lock()
active_connections: dict[str, WebSocket] = {}
cache_manager: CacheManager = None
PRE_WARM_KEYWORDS = ["wireless mouse", "bluetooth headphones"] # ìºì‹œ ì›Œë° í‚¤ì›Œë“œ

# --- Pydantic ëª¨ë¸ ---
class CacheClearRequest(BaseModel):
    keyword: str

# --- ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ---
async def warm_up_cache():
    """ì„œë²„ ì‹œì‘ ì‹œ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìºì‹œë¥¼ ë¯¸ë¦¬ ì±„ì›Œë„£ìŠµë‹ˆë‹¤."""
    print("ğŸ”¥ Starting cache warm-up in background...")
    await asyncio.sleep(5) # ì„œë²„ê°€ ì™„ì „íˆ ì•ˆì •ë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°

    for keyword in PRE_WARM_KEYWORDS:
        async with scraping_lock:
            if cache_manager and cache_manager.get_analysis_result(keyword):
                print(f"âœ… Cache for '{keyword}' already exists. Skipping warm-up.")
                continue
            
            print(f"ğŸ”¥ Warming up cache for '{keyword}'...")
            try:
                # ìŠ¤í¬ë ˆì´í•‘ ë° ë¶„ì„
                await scraper.scrape_and_save_to_db(keyword, max_products=30)
                competition_report = sqlite_analyzer.analyze_category_competition(keyword)
                saturation_report = sqlite_analyzer.calculate_market_saturation(keyword)
                report_data = {**competition_report, **saturation_report}
                report_data['keyword'] = keyword

                # L2 ìºì‹œì— ì €ì¥
                if cache_manager:
                    cache_manager.set_analysis_result(keyword, report_data, ttl_hours=24)
                print(f"âœ… Cache warmed up for '{keyword}'.")
            except Exception as e:
                print(f"âŒ Error warming up cache for '{keyword}': {e}")

# --- ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ---
@app.on_event("startup")
async def startup_event():
    global cache_manager
    try:
        cache_manager = get_cache_manager()
        print("âœ… Redis Cache Manager connected.")
    except Exception as e:
        print(f"âŒ Redis connection failed: {e}")
        cache_manager = None
    
    await scraper.start_browser()
    # ìºì‹œ ì›Œë° ì‘ì—…ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰
    asyncio.create_task(warm_up_cache())

@app.on_event("shutdown")
async def shutdown_event():
    await scraper.close_browser()

# --- WebSocket ë¡œì§ (ìƒëµ, ì´ì „ê³¼ ë™ì¼) ---
async def send_progress(client_id: str, progress: int, message: str, status: str = "processing"):
    if client_id in active_connections:
        await active_connections[client_id].send_text(json.dumps({"progress": progress, "message": message, "status": status}))

@app.websocket("/ws/{client_id}")
async def websocket_endpoint(websocket: WebSocket, client_id: str):
    await websocket.accept()
    active_connections[client_id] = websocket
    try:
        while True:
            data = await websocket.receive_text()
            data_json = json.loads(data)
            keyword = data_json.get('keyword')
            if keyword:
                asyncio.create_task(run_analysis_job(client_id, keyword))
    except Exception:
        del active_connections[client_id]

async def run_analysis_job(client_id: str, keyword: str):
    async with scraping_lock:
        try:
            if cache_manager and cache_manager.get_analysis_result(keyword):
                await send_progress(client_id, 100, "Report ready!", "completed")
                return
            await send_progress(client_id, 10, "Starting market analysis...")
            db_result = await scraper.scrape_and_save_to_db(keyword, max_products=30)
            if not db_result or not db_result.get('success'):
                await send_progress(client_id, 100, db_result.get('message', 'Scraping error'), "error")
                return
            await send_progress(client_id, 70, "Analysis complete. Generating report...")
            await asyncio.sleep(1)
            await send_progress(client_id, 100, "Report ready!", "completed")
        except Exception as e:
            await send_progress(client_id, 100, f"An unexpected error occurred: {e}", "error")

# --- ì›¹ í˜ì´ì§€ ë° API ë¼ìš°íŒ… ---
@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/report", response_class=HTMLResponse)
async def get_report(request: Request, keyword: str):
    try:
        if cache_manager:
            cached_report = cache_manager.get_analysis_result(keyword)
            if cached_report:
                return templates.TemplateResponse("report.html", {"request": request, "report": cached_report})
        
        competition_report = sqlite_analyzer.analyze_category_competition(keyword)
        saturation_report = sqlite_analyzer.calculate_market_saturation(keyword)
        report_data = {**competition_report, **saturation_report}
        report_data['keyword'] = keyword

        if cache_manager:
            cache_manager.set_analysis_result(keyword, report_data, ttl_hours=24)

        return templates.TemplateResponse("report.html", {"request": request, "report": report_data})
    except Exception as e:
        return templates.TemplateResponse("error.html", {"request": request, "error_message": f"Error generating report: {e}"})

@app.post("/api/cache/clear")
async def clear_cache(payload: CacheClearRequest):
    keyword = payload.keyword
    l1_cleared = False
    l2_cleared = False
    try:
        sqlite_analyzer.analyze_category_competition.cache_clear()
        sqlite_analyzer.calculate_market_saturation.cache_clear()
        l1_cleared = True
    except Exception as e:
        print(f"âš ï¸ Failed to clear L1 cache: {e}")
    if cache_manager:
        try:
            l2_cleared = cache_manager.delete_analysis_result(keyword)
        except Exception as e:
            print(f"âš ï¸ Failed to clear L2 cache for '{keyword}': {e}")
    if l1_cleared or l2_cleared:
        return JSONResponse(content={"message": f"Cache for '{keyword}' cleared.", "l1_cleared": l1_cleared, "l2_cleared": l2_cleared})
    else:
        raise HTTPException(status_code=500, detail="Failed to clear any cache.")
