# -*- coding: utf-8 -*-
"""
Amazon Market Insights Pro - Scraper Module
Amazon ìƒí’ˆ ë°ì´í„° ìŠ¤í¬ë˜í•‘ì„ ë‹´ë‹¹í•˜ëŠ” AmazonScraper í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì  ì›¹ í˜ì´ì§€ë¥¼ ë¡œë“œí•˜ê³ , BeautifulSoupìœ¼ë¡œ ë°ì´í„°ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
"""
import asyncio
import csv
import os
import random
from datetime import datetime
from playwright.async_api import async_playwright, Page, Browser, TimeoutError
from bs4 import BeautifulSoup

# ORM ëª¨ë¸ import
try:
    from .models import db_manager, Product, ScrapingSession
except ImportError:
    # ì§ì ‘ ì‹¤í–‰ì‹œì—ëŠ” ì ˆëŒ€ import ì‚¬ìš©
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(__file__)))
    from core.models import db_manager, Product, ScrapingSession

class AmazonScraper:
    """
    Amazon ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìƒí’ˆ ë°ì´í„°ë¥¼ ìŠ¤í¬ë ˆì´í•‘í•˜ëŠ” í´ë˜ìŠ¤.
    ë´‡ íƒì§€ ìš°íšŒ ë° Amazon íŠ¹í™” ìµœì í™”ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.
    """
    def __init__(self):
        self.browser: Browser | None = None
        self.page: Page | None = None

    async def start_browser(self):
        """Playwrightë¥¼ ì‹œì‘í•˜ê³  ë¸Œë¼ìš°ì €ì™€ í˜ì´ì§€ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        print("ë¸Œë¼ìš°ì €ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ (Chromium ì‚¬ìš©)...")
        pw = await async_playwright().start()
        
        self.browser = await pw.chromium.launch(
            headless=False,  # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ë¹„í™œì„±í™” (ë” ìì—°ìŠ¤ëŸ½ê²Œ)
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-http2',  # HTTP/2 ë¹„í™œì„±í™”
                '--disable-blink-features=AutomationControlled',
                '--disable-features=VizDisplayCompositor',
                '--disable-web-security',
                '--disable-features=site-per-process',
                '--no-first-run',
                '--disable-extensions',
                '--disable-automation',
                '--disable-infobars',
                '--start-maximized',
                # Amazon íŠ¹í™” ì¶”ê°€ ì„¤ì •
                '--disable-notifications',
                '--disable-popup-blocking',
                '--disable-background-timer-throttling',
                '--disable-backgrounding-occluded-windows',
                '--disable-features=TranslateUI',
                '--disable-ipc-flooding-protection',
                '--disable-features=VizDisplayCompositor,site-per-process',
                '--disable-field-trial-config',
                '--disable-plugins-discovery',
                '--disable-default-apps',
                '--no-default-browser-check',
                '--disable-component-extensions-with-background-pages'
            ]
        )
        
        # ë‹¤ì–‘í•œ ì‹¤ì œ User-Agent ë¡œí…Œì´ì…˜
        import random
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0"
        ]
        selected_user_agent = random.choice(user_agents)
        print(f"ğŸ”„ User-Agent ì„ íƒ: {selected_user_agent[:50]}...")
        
        context = await self.browser.new_context(
            user_agent=selected_user_agent,
            viewport={'width': 1920, 'height': 1080},
            locale='en-US',  # í•œêµ­ì–´ì—ì„œ ì˜ì–´ë¡œ
            timezone_id='America/New_York',  # ë¯¸êµ­ ë™ë¶€ ì‹œê°„ëŒ€
            ignore_https_errors=True,
            java_script_enabled=True,
            permissions=['geolocation'],
            # ì•„ë§ˆì¡´ íŠ¹í™” í—¤ë” (ë” ìì—°ìŠ¤ëŸ½ê²Œ)
            extra_http_headers={
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Cache-Control': 'max-age=0'
            }
        )
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì • - Amazonì€ ë” ë¹ ë¥´ê²Œ ë°˜ì‘í•˜ë¯€ë¡œ ì¤„ì„
        context.set_default_timeout(90000)  # 1.5ë¶„
        
        self.page = await context.new_page()
        
        # Amazon íŠ¹í™” ë´‡ íƒì§€ ìš°íšŒ ìŠ¤í¬ë¦½íŠ¸ ì£¼ì…
        await self.page.add_init_script("""
            // navigator.webdriver ì œê±°
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined,
            });
            
            // í”ŒëŸ¬ê·¸ì¸ ì •ë³´ ì¶”ê°€
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5],
            });
            
            // ì–¸ì–´ ì„¤ì •
            Object.defineProperty(navigator, 'languages', {
                get: () => ['en-US', 'en'],
            });
            
            // ìŠ¤í¬ë¦° í•´ìƒë„ ìì—°ìŠ¤ëŸ½ê²Œ
            Object.defineProperty(screen, 'width', {
                get: () => 1920,
            });
            Object.defineProperty(screen, 'height', {
                get: () => 1080,
            });
            
            // Permission API ëª¨í‚¹
            const originalQuery = window.navigator.permissions.query;
            window.navigator.permissions.query = (parameters) => (
                parameters.name === 'notifications' ?
                    Promise.resolve({ state: Notification.permission }) :
                    originalQuery(parameters)
            );
            
            // Chrome runtime ì •ë³´
            window.chrome = window.chrome || {};
            window.chrome.runtime = window.chrome.runtime || {};
            window.chrome.runtime.onConnect = {
                addListener: () => {},
                removeListener: () => {},
            };
        """)
        
        print("ë¸Œë¼ìš°ì €ê°€ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # Amazon ì„¸ì…˜ ì›Œë°ì—… - ì‹¤ì œ ì‚¬ìš©ìì²˜ëŸ¼ í–‰ë™
        print("ğŸ”„ Amazon ì„¸ì…˜ ì´ˆê¸°í™” ì¤‘...")
        try:
            # ë¨¼ì € Amazon í™ˆí˜ì´ì§€ì— ë°©ë¬¸í•˜ì—¬ ì •ìƒì ì¸ ì„¸ì…˜ ìƒì„±
            await self.page.goto("https://www.amazon.com", wait_until='domcontentloaded', timeout=30000)
            
            # ì¸ê°„ì²˜ëŸ¼ í˜ì´ì§€ë¥¼ ì‚´í´ë³´ëŠ” ì‹œê°„
            warmup_delay = random.uniform(3, 7)  # 3-7ì´ˆ ëœë¤ ëŒ€ê¸°
            print(f"â±ï¸ ì„¸ì…˜ ì›Œë°ì—…: {warmup_delay:.1f}ì´ˆ ëŒ€ê¸°...")
            await asyncio.sleep(warmup_delay)
            
            # í˜ì´ì§€ë¥¼ ì•½ê°„ ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ìì—°ìŠ¤ëŸ½ê²Œ
            await self.page.evaluate("window.scrollTo(0, Math.random() * 500)")
            await asyncio.sleep(random.uniform(1, 2))
            
            print("âœ… Amazon ì„¸ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ì„¸ì…˜ ì›Œë°ì—… ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")

    async def close_browser(self):
        """ë¸Œë¼ìš°ì €ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤."""
        if self.browser:
            await self.browser.close()
            print("ë¸Œë¼ìš°ì €ë¥¼ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤.")

    async def scrape_search_page(self, keyword: str):
        """
        [v3] ì‚¬ëŒì²˜ëŸ¼ í–‰ë™í•˜ì—¬ ì£¼ì–´ì§„ í‚¤ì›Œë“œë¡œ ì¿ íŒ¡ì„ ê²€ìƒ‰í•˜ê³  ìƒí’ˆ ëª©ë¡ì„ ìŠ¤í¬ë ˆì´í•‘í•©ë‹ˆë‹¤.
        """
        if not self.page:
            return {"status": "error", "reason": "ë¸Œë¼ìš°ì €ê°€ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

        print(f"'{keyword}' í‚¤ì›Œë“œë¡œ ìŠ¤í¬ë ˆì´í•‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        import random  # random ëª¨ë“ˆ ì¶”ê°€
        
        try:
            # ë°©ë²• 1: ì§ì ‘ ê²€ìƒ‰ URLë¡œ ì´ë™ (ì•„ë§ˆì¡´)
            import urllib.parse
            encoded_keyword = urllib.parse.quote(keyword)
            search_url = f"https://www.amazon.com/s?k={encoded_keyword}"
            
            print(f"ğŸ¯ ì§ì ‘ ê²€ìƒ‰ URL ì ‘ì†: {search_url}")
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    print(f"ğŸ“‹ ê²€ìƒ‰ í˜ì´ì§€ ì ‘ì† ì‹œë„ {attempt + 1}/{max_retries}...")
                    # ì¸ê°„ì²˜ëŸ¼ í–‰ë™: ë” ê¸´ ëŒ€ê¸° ì‹œê°„ìœ¼ë¡œ Amazon ì°¨ë‹¨ ìš°íšŒ
                    base_delay = 5 + attempt * 3  # 5ì´ˆ, 8ì´ˆ, 11ì´ˆ... (ì°¨ë‹¨ ìš°íšŒë¥¼ ìœ„í•´ ì¦ê°€)
                    random_delay = random.uniform(2, 5)  # 2~5ì´ˆ ëœë¤ ì¶”ê°€
                    total_delay = base_delay + random_delay
                    print(f"â±ï¸ Amazon ì°¨ë‹¨ ìš°íšŒë¥¼ ìœ„í•œ ëŒ€ê¸°: {total_delay:.1f}ì´ˆ...")
                    await asyncio.sleep(total_delay)
                    
                    await self.page.goto(search_url, wait_until='domcontentloaded', timeout=60000)
                    
                    # í˜ì´ì§€ ë¡œë”© í›„ ì¶”ê°€ ëŒ€ê¸° (Amazonì€ ë” ë¹ ë¥´ê²Œ)
                    await asyncio.sleep(2 + attempt)  # 2ì´ˆ, 3ì´ˆ, 4ì´ˆ...
                    print("âœ… ê²€ìƒ‰ í˜ì´ì§€ ì ‘ì† ì„±ê³µ!")
                    break
                except Exception as retry_error:
                    print(f"âš ï¸ ì§ì ‘ ì ‘ì† ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {retry_error}")
                    if attempt == max_retries - 1:
                        print("ğŸ”„ ë” ìì—°ìŠ¤ëŸ¬ìš´ ê²€ìƒ‰ ë°©ì‹ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤...")
                        # ë°©ë²• 2: ë§¤ìš° ìì—°ìŠ¤ëŸ¬ìš´ ë°©ì‹ (ì•„ë§ˆì¡´ ë©”ì¸ í˜ì´ì§€ -> ê²€ìƒ‰)
                        await self.page.goto("https://www.amazon.com/", wait_until='domcontentloaded', timeout=60000)
                        
                        # í™ˆí˜ì´ì§€ì—ì„œ ì ê¹ ë¨¸ë¬´ë¥´ê¸°
                        await asyncio.sleep(random.uniform(3, 6))
                        
                        # í˜ì´ì§€ë¥¼ ì¡°ê¸ˆ ìŠ¤í¬ë¡¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ
                        await self.page.evaluate("window.scrollTo(0, Math.random() * 300)")
                        await asyncio.sleep(random.uniform(1, 2))
                        
                        search_input = await self.page.wait_for_selector("input#twotabsearchtextbox", timeout=20000)
                        await search_input.click()
                        
                        # ê²€ìƒ‰ì°½ í´ë¦¬ì–´í•˜ê¸° ì „ì— ì ê¹ ëŒ€ê¸°
                        await asyncio.sleep(random.uniform(0.5, 1))
                        await search_input.clear()
                        
                        # ì¸ê°„ì²˜ëŸ¼ ì²œì²œíˆ íƒ€ì´í•‘ (ë” ê¸´ ì§€ì—°)
                        typing_delay = random.randint(80, 150)  # 80-150ms ì§€ì—°
                        await search_input.type(keyword, delay=typing_delay)
                        
                        # íƒ€ì´í•‘ í›„ ì ê¹ ëŒ€ê¸° (ì‚¬ìš©ìê°€ ìƒê°í•˜ëŠ” ì‹œê°„)
                        await asyncio.sleep(random.uniform(1, 3))
                        
                        search_button = await self.page.wait_for_selector("input#nav-search-submit-button", timeout=10000)
                        await search_button.click()
                        
                        await self.page.wait_for_load_state('domcontentloaded', timeout=30000)
                        
                        # ê²€ìƒ‰ ê²°ê³¼ ë¡œë”© í›„ ì¶”ê°€ ëŒ€ê¸°
                        await asyncio.sleep(random.uniform(4, 7))
                        break
                    await asyncio.sleep(3)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°

        except Exception as e:
            print(f"âŒ ìŠ¤í¬ë ˆì´í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {"status": "error", "reason": f"ì•„ë§ˆì¡´ í˜ì´ì§€ì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"}

        # 5. Amazon ì—ëŸ¬ í˜ì´ì§€ ê°ì§€
        print("ğŸ” Amazon ì—ëŸ¬ í˜ì´ì§€ í™•ì¸ ì¤‘...")
        page_title = await self.page.title()
        page_url = self.page.url
        
        # Amazon ì—ëŸ¬ í˜ì´ì§€ ê°ì§€
        error_indicators = [
            "Sorry! Something went wrong!",
            "503 Service Unavailable", 
            "500 Internal Server Error",
            "Robot Check",
            "captcha"
        ]
        
        page_content = await self.page.content()
        for indicator in error_indicators:
            if indicator.lower() in page_title.lower() or indicator.lower() in page_content.lower():
                print(f"âŒ Amazon ì—ëŸ¬ í˜ì´ì§€ ê°ì§€: {indicator}")
                return {"status": "error", "reason": f"Amazonì´ ì—ëŸ¬ í˜ì´ì§€ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤: {indicator}. í‚¤ì›Œë“œë¥¼ ë³€ê²½í•˜ê±°ë‚˜ ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."}
        
        # 6. ì•„ë§ˆì¡´ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ì•„ë§ˆì¡´ì€ ë” ê°„ë‹¨)
        print("â° ì•„ë§ˆì¡´ ìƒí’ˆ ëª©ë¡ ë¡œë”© ëŒ€ê¸° ì¤‘...")
        
        # ì•„ë§ˆì¡´ ìƒí’ˆ ì»¨í…Œì´ë„ˆ ì„ íƒìë“¤ì„ ë°˜ë³µ ì²´í¬
        product_container_selectors = [
            '[data-component-type="s-search-result"]',  # ì•„ë§ˆì¡´ ê²€ìƒ‰ ê²°ê³¼
            '.s-result-item',  # ì•„ë§ˆì¡´ ìƒí’ˆ ì•„ì´í…œ
            '[data-asin]',  # ASIN ì†ì„±ì„ ê°€ì§„ ìš”ì†Œ
            '.s-card-container',  # ì•„ë§ˆì¡´ ì¹´ë“œ ì»¨í…Œì´ë„ˆ
            '[data-cy="title-recipe-list"] > div',  # ì•„ë§ˆì¡´ ìƒí’ˆ ëª©ë¡
            '.s-widget-container .s-card-container',  # ìƒí’ˆ ìœ„ì ¯ ì»¨í…Œì´ë„ˆ
            '[data-cel-widget*="search_result"]',  # ê²€ìƒ‰ ê²°ê³¼ ìœ„ì ¯
        ]
        
        # ìµœëŒ€ 10ì´ˆ ë™ì•ˆ 1ì´ˆë§ˆë‹¤ ìƒí’ˆì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸ (ì•„ë§ˆì¡´ì€ ë” ë¹ ë¦„)
        products_found = False
        for attempt in range(10):
            print(f"  ìƒí’ˆ ë¡œë”© í™•ì¸ {attempt + 1}/10...")
            
            for selector in product_container_selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    if elements and len(elements) > 0:
                        print(f"âœ… ìƒí’ˆ ë°œê²¬! '{selector}' ì„ íƒìë¡œ {len(elements)}ê°œ ìš”ì†Œ ì°¾ìŒ")
                        products_found = True
                        break
                except:
                    continue
            
            if products_found:
                break
                
            await asyncio.sleep(1)  # 1ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
        
        if not products_found:
            print("âŒ 10ì´ˆ ë™ì•ˆ ìƒí’ˆ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")
            
        # 6. ì•„ë§ˆì¡´ ìƒí’ˆ ëª©ë¡ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ê¸° ìœ„í•œ ì„ íƒìë“¤
        selectors_to_try = [
            # ì•„ë§ˆì¡´ ê²€ìƒ‰ ê²°ê³¼ ì»¨í…Œì´ë„ˆ
            '[data-cy="title-recipe-list"]',  # ì•„ë§ˆì¡´ ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡
            '.s-search-results',  # ê²€ìƒ‰ ê²°ê³¼ ì»¨í…Œì´ë„ˆ
            '[data-component-type="s-search-result"]',  # ê²€ìƒ‰ ê²°ê³¼ ì»´í¬ë„ŒíŠ¸
            '.s-widget-container',  # ìœ„ì ¯ ì»¨í…Œì´ë„ˆ
            '#search',  # ê²€ìƒ‰ ì„¹ì…˜
            
            # ë°±ì—… ì„ íƒì
            '[cel_widget_id="MAIN-SEARCH_RESULTS"]',  # ë©”ì¸ ê²€ìƒ‰ ê²°ê³¼
            '.s-card-container',  # ì¹´ë“œ ì»¨í…Œì´ë„ˆ
            '[data-asin]',  # ASIN ìš”ì†Œë“¤ì˜ ë¶€ëª¨
        ]
        
        product_list_container = None
        for selector in selectors_to_try:
            try:
                print(f"ğŸ” ì„ íƒì '{selector}' ì‹œë„ ì¤‘...")
                product_list_container = await self.page.query_selector(selector)
                if product_list_container:
                    print(f"âœ… ì„ íƒì '{selector}'ë¡œ ìƒí’ˆ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                    break
                else:
                    print(f"âŒ ì„ íƒì '{selector}'ë¡œ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            except Exception as e:
                error_msg = str(e)
                if "Target page, context or browser has been closed" in error_msg:
                    print(f"âŒ ë¸Œë¼ìš°ì €ê°€ ì˜ˆê¸°ì¹˜ ì•Šê²Œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤: {e}")
                    return {"status": "error", "reason": "ë¸Œë¼ìš°ì €ê°€ Amazonì— ì˜í•´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë„ˆë¬´ ë¹ ë¥¸ ìš”ì²­ìœ¼ë¡œ ì¸í•œ ì°¨ë‹¨ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤."}
                print(f"âš ï¸ ì„ íƒì '{selector}' ì˜¤ë¥˜: {e}")
                continue
                
        if not product_list_container:
            try:
                html = await self.page.content()
                with open("error_page.html", "w", encoding="utf-8") as f:
                    f.write(html)
            except Exception as e:
                if "Target page, context or browser has been closed" in str(e):
                    return {"status": "error", "reason": "ë¸Œë¼ìš°ì €ê°€ Amazonì— ì˜í•´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë„ˆë¬´ ë¹ ë¥¸ ìš”ì²­ìœ¼ë¡œ ì¸í•œ ì°¨ë‹¨ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤."}
            return {"status": "error", "reason": "ìƒí’ˆ ëª©ë¡ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (error_page.html ì €ì¥ë¨)"}

        # Next.js ë Œë”ë§ëœ ì»¨í…Œì´ë„ˆì˜ HTML ì¶”ì¶œ
        html = await product_list_container.inner_html()
        soup = BeautifulSoup(html, "html.parser")
        
        # ì•„ë§ˆì¡´ êµ¬ì¡°ì—ì„œ ìƒí’ˆ ì•„ì´í…œ íŒ¨í„´ ì‹œë„
        products = []
        product_selectors = [
            "[data-component-type='s-search-result']",  # ì•„ë§ˆì¡´ ê²€ìƒ‰ ê²°ê³¼ ì•„ì´í…œ
            ".s-result-item",  # ê²€ìƒ‰ ê²°ê³¼ ì•„ì´í…œ
            "[data-asin]",  # ASIN ì†ì„±ì„ ê°€ì§„ ìš”ì†Œ
            ".s-card-container",  # ì¹´ë“œ ì»¨í…Œì´ë„ˆ
            "[data-cel-widget*='search_result']",  # ê²€ìƒ‰ ê²°ê³¼ ìœ„ì ¯
            "div[data-cy='title-recipe-list'] > div",  # ìƒí’ˆ ëª©ë¡ì˜ ê°œë³„ ì•„ì´í…œ
        ]
        
        for selector in product_selectors:
            products = soup.select(selector)
            if products:
                print(f"âœ… '{selector}' ì„ íƒìë¡œ {len(products)}ê°œ ìƒí’ˆ ìš”ì†Œ ì°¾ìŒ")
                break
        
        if not products:
            # ë””ë²„ê·¸: ì‹¤ì œ HTML êµ¬ì¡° ì €ì¥
            with open("debug_container.html", "w", encoding="utf-8") as f:
                f.write(html[:5000])  # ì²˜ìŒ 5000ìë§Œ ì €ì¥
            return {"status": "error", "reason": "ìƒí’ˆ ëª©ë¡ ì˜ì—­ì€ ì°¾ì•˜ìœ¼ë‚˜, ë‚´ë¶€ì— ê°œë³„ ìƒí’ˆ ìš”ì†Œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (debug_container.html ì €ì¥ë¨)"}

        print(f"{len(products)}ê°œì˜ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        scraped_data = []
        import re
        
        for i, product in enumerate(products):
            name, price, product_url = "N/A", 0, ""
            
            # URL ì¶”ì¶œ (ì•„ë§ˆì¡´ íŒ¨í„´)
            url_element = product.find("a", href=True)
            if url_element and url_element.get('href'):
                href = url_element['href']
                product_url = "https://www.amazon.com" + href if href.startswith('/') else href
            
            # ìƒí’ˆëª… ì¶”ì¶œ (ì•„ë§ˆì¡´ íŒ¨í„´) - ê°œì„ ëœ ë¡œì§
            name_selectors = [
                "h3.s-size-mini a span",  # ì•„ë§ˆì¡´ ìƒí’ˆ ì œëª© (ê°€ì¥ ì •í™•)
                "[data-cy='title-recipe-list'] h3 a span",  # ì•„ë§ˆì¡´ ì œëª©
                "h2 a span",  # h2 ë‚´ì˜ ë§í¬ ìŠ¤íŒ¬
                "h3 a span",  # h3 ë‚´ì˜ ë§í¬ ìŠ¤íŒ¬
                ".s-size-mini a span",  # ë¯¸ë‹ˆ ì‚¬ì´ì¦ˆ ë‚´ì˜ ë§í¬ ìŠ¤íŒ¬
                "a[aria-label]",  # aria-labelì´ ìˆëŠ” ë§í¬ (ë°±ì—…)
                "img[alt]",  # ì´ë¯¸ì§€ alt ì†ì„± (ìµœí›„ ë°±ì—…)
            ]
            
            for selector in name_selectors:
                element = product.select_one(selector)
                if element:
                    if element.name == "img" and element.get('alt'):
                        candidate_name = element['alt'].strip()
                    elif element.name == "a" and element.get('aria-label'):
                        candidate_name = element.get('aria-label').strip()
                    else:
                        candidate_name = element.get_text().strip()
                    
                    # ì˜ëª»ëœ í…ìŠ¤íŠ¸ í•„í„°ë§
                    invalid_texts = [
                        'sponsored', 'best seller', 'amazon\'s choice', 'overall pick',
                        'limited time deal', '#1 best seller', 'climate pledge friendly',
                        'add to cart', 'save', 'coupon', 'free shipping'
                    ]
                    
                    if (candidate_name and 
                        candidate_name != "N/A" and 
                        len(candidate_name) > 10 and  # ìµœì†Œ ê¸¸ì´ í™•ë³´
                        not any(invalid in candidate_name.lower() for invalid in invalid_texts)):
                        name = candidate_name
                        break
            
            # ê°€ê²© ì¶”ì¶œ (ì•„ë§ˆì¡´ ë‹¬ëŸ¬ êµ¬ì¡°)
            price_selectors = [
                ".a-price-whole",  # ì•„ë§ˆì¡´ ê°€ê²© (ì •ìˆ˜ ë¶€ë¶„)
                ".a-price .a-offscreen",  # ì•„ë§ˆì¡´ ìˆ¨ê²¨ì§„ ê°€ê²©
                ".a-price-range",  # ê°€ê²© ë²”ìœ„
                "[data-cy='price-recipe'] .a-price",  # ê°€ê²© ë ˆì‹œí”¼
                ".s-price-instructions-style .a-price",  # ê°€ê²© ì§€ì‹œ
                ".a-color-price",  # ê°€ê²© ìƒ‰ìƒ
                ".a-size-base.a-color-price",  # ê¸°ë³¸ ê°€ê²©
            ]
            
            for selector in price_selectors:
                price_element = product.select_one(selector)
                if price_element:
                    price_text = price_element.get_text().strip()
                    # ì•„ë§ˆì¡´ ë‹¬ëŸ¬ ê°€ê²© íŒ¨í„´: $12.99 ë˜ëŠ” 12.99
                    price_match = re.search(r'[\$]?([\d,]+\.?\d*)', price_text.replace(',', ''))
                    if price_match:
                        price = float(price_match.group(1))
                        break
            
            # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì¶”ê°€
            if name not in ["N/A", ""] and price > 0 and product_url:
                scraped_data.append({"name": name, "price": price, "url": product_url})
                if len(scraped_data) <= 3:  # ì²˜ìŒ 3ê°œë§Œ ë””ë²„ê·¸ ì¶œë ¥
                    print(f"  ìƒí’ˆ {len(scraped_data)}: {name[:30]}... - {price:,}ì›")
            elif i < 5:  # ì²˜ìŒ 5ê°œ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ë§Œ ë””ë²„ê·¸ ì¶œë ¥
                print(f"  ìƒí’ˆ {i+1} íŒŒì‹± ì‹¤íŒ¨: name='{name[:20]}...' price={price} url='{product_url[:30]}...'")
        
        
        if not scraped_data:
            return {"status": "error", "reason": f"{len(products)}ê°œì˜ ìƒí’ˆ ì˜ì—­ì„ ë¶„ì„í–ˆìœ¼ë‚˜, ìœ íš¨í•œ ì´ë¦„ê³¼ ê°€ê²© ì •ë³´ë¥¼ ê°€ì§„ ìƒí’ˆì„ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}

        print(f"ì„±ê³µì ìœ¼ë¡œ {len(scraped_data)}ê°œì˜ ìƒí’ˆ ë°ì´í„°ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
        return {"status": "success", "data": scraped_data}

    async def scrape_product_detail(self, product_url: str):
        """
        ê°œë³„ ìƒí’ˆ í˜ì´ì§€ì—ì„œ ìƒì„¸ ì •ë³´ë¥¼ ìŠ¤í¬ë ˆì´í•‘í•©ë‹ˆë‹¤.
        """
        if not self.page:
            raise Exception("ë¸Œë¼ìš°ì €ê°€ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. start_browser()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

        print(f"ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ ìŠ¤í¬ë ˆì´í•‘ì„ ì‹œì‘í•©ë‹ˆë‹¤: {product_url}")
        
        try:
            await self.page.goto(product_url, wait_until='domcontentloaded', timeout=15000)
            
            # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
            await asyncio.sleep(2)
            
            html = await self.page.content()
            soup = BeautifulSoup(html, "html.parser")
            
            print("=== ìƒì„¸ í˜ì´ì§€ ìš”ì†Œ ì¶”ì¶œ ì‹œì‘ ===")
            
            # ìƒí’ˆëª… (ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„)
            detail_name = "N/A"
            name_selectors = [
                "h1.prod-buy-header__title",
                "h1.prod-title", 
                ".prod-buy-header__title",
                "h1",
                "title"
            ]
            
            for selector in name_selectors:
                name_element = soup.select_one(selector)
                if name_element:
                    if selector == "title":
                        detail_name = name_element.text.strip().split(' - ì¿ íŒ¡!')[0]
                    else:
                        detail_name = name_element.text.strip()
                    if detail_name and detail_name != "N/A":
                        print(f"ìƒí’ˆëª… ë°œê²¬ (ì…€ë ‰í„°: {selector}): {detail_name[:50]}...")
                        break
            
            # ë¸Œëœë“œ ì •ë³´
            brand = "N/A"
            brand_selectors = [
                "a.prod-brand-name",
                ".brand-name",
                ".prod-brand",
                "[class*='brand']"
            ]
            
            for selector in brand_selectors:
                brand_element = soup.select_one(selector)
                if brand_element:
                    brand = brand_element.text.strip()
                    if brand and brand != "N/A":
                        print(f"ë¸Œëœë“œ ë°œê²¬ (ì…€ë ‰í„°: {selector}): {brand}")
                        break
            
            # íŒë§¤ì ì •ë³´  
            seller = "N/A"
            seller_selectors = [
                "a.shop-name",
                ".shop-name",
                ".seller-name",
                "[class*='shop']",
                "[class*='seller']"
            ]
            
            for selector in seller_selectors:
                seller_element = soup.select_one(selector)
                if seller_element:
                    seller = seller_element.text.strip()
                    if seller and seller != "N/A":
                        print(f"íŒë§¤ì ë°œê²¬ (ì…€ë ‰í„°: {selector}): {seller}")
                        break
            
            # Prime ë°°ì†¡ ì—¬ë¶€ (ì•„ë§ˆì¡´)
            prime_selectors = [
                ".a-icon-prime",
                "[aria-label*='Prime']",
                "[alt*='Prime']",
                ".s-prime",
                "[class*='prime']"
            ]
            is_prime = False
            for selector in prime_selectors:
                prime_element = soup.select_one(selector)
                if prime_element:
                    is_prime = True
                    print(f"Prime ë°°ì†¡ ë°œê²¬ (ì…€ë ‰í„°: {selector})")
                    break
            
            # í‰ì  ì •ë³´
            rating = 0.0
            rating_selectors = [
                "span.rating-star-num",
                ".rating-star-num",
                ".rating",
                "[class*='rating']",
                "[class*='star']"
            ]
            
            for selector in rating_selectors:
                rating_element = soup.select_one(selector)
                if rating_element:
                    rating_text = rating_element.text.strip()
                    try:
                        rating = float(rating_text)
                        print(f"í‰ì  ë°œê²¬ (ì…€ë ‰í„°: {selector}): {rating}")
                        break
                    except ValueError:
                        continue
            
            # ë¦¬ë·° ìˆ˜
            review_count = 0
            review_selectors = [
                "span.count",
                ".count",
                ".review-count", 
                "[class*='review']",
                "[class*='count']"
            ]
            
            for selector in review_selectors:
                review_element = soup.select_one(selector)
                if review_element:
                    review_text = review_element.text.strip().replace("(", "").replace(")", "").replace(",", "")
                    try:
                        review_count = int(''.join(filter(str.isdigit, review_text)))
                        if review_count > 0:
                            print(f"ë¦¬ë·°ìˆ˜ ë°œê²¬ (ì…€ë ‰í„°: {selector}): {review_count}")
                            break
                    except ValueError:
                        continue
            
            # ê°€ê²© ì •ë³´
            price = 0
            price_selectors = [
                "span.total-price strong",
                ".total-price strong",
                ".price strong",
                ".price-value",
                "[class*='price']",
                "strong"
            ]
            
            for selector in price_selectors:
                price_element = soup.select_one(selector)
                if price_element:
                    price_text = price_element.text.strip().replace(",", "").replace("ì›", "")
                    try:
                        price = int(float(price_text))
                        if price > 0:
                            print(f"ê°€ê²© ë°œê²¬ (ì…€ë ‰í„°: {selector}): {price}ì›")
                            break
                    except ValueError:
                        continue
            
            product_detail = {
                "detail_name": detail_name,
                "brand": brand,
                "seller": seller,
                "is_prime": is_prime,
                "rating": rating,
                "review_count": review_count,
                "price": price,
                "url": product_url
            }
            
            print(f"ìƒí’ˆ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ: {detail_name}")
            return product_detail
            
        except Exception as e:
            print(f"ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ ìŠ¤í¬ë ˆì´í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {
                "detail_name": "ERROR",
                "brand": "N/A",
                "seller": "N/A", 
                "is_prime": False,
                "rating": 0.0,
                "review_count": 0,
                "price": 0,
                "url": product_url
            }

    def save_to_csv(self, products_data, filename=None):
        """
        ìˆ˜ì§‘í•œ ìƒí’ˆ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        analyzer.pyì™€ í˜¸í™˜ë˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
        """
        if not products_data:
            print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # íŒŒì¼ëª… ìƒì„± (ê¸°ë³¸ê°’: í˜„ì¬ ì‹œê°„)
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"coupang_products_{timestamp}.csv"
        
        # data í´ë” í™•ì¸ ë° ìƒì„±
        data_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data')
        os.makedirs(data_dir, exist_ok=True)
        
        filepath = os.path.join(data_dir, filename)
        
        # analyzer.pyì™€ í˜¸í™˜ë˜ëŠ” CSV í—¤ë” ì •ì˜
        fieldnames = [
            'product_id',           # ìƒí’ˆ ê³ ìœ  ID
            'product_title',        # ìƒí’ˆëª…  
            'product_category',     # ì¹´í…Œê³ ë¦¬ (ê²€ìƒ‰ í‚¤ì›Œë“œë¡œ ëŒ€ì²´)
            'discounted_price',     # í• ì¸ ê°€ê²©
            'product_rating',       # í‰ì 
            'total_reviews',        # ë¦¬ë·° ì´ ê°œìˆ˜
            'purchased_last_month', # ì§€ë‚œë‹¬ êµ¬ë§¤ìˆ˜ (ì„ì˜ê°’)
            'brand',               # ë¸Œëœë“œ
            'seller',              # íŒë§¤ì
            'is_prime',           # Prime ë°°ì†¡ ì—¬ë¶€
            'scraped_at'           # ìˆ˜ì§‘ ì‹œê°„
        ]
        
        print(f"CSV íŒŒì¼ë¡œ ì €ì¥ ì¤‘: {filepath}")
        
        with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for i, product in enumerate(products_data):
                # ì¿ íŒ¡ ë°ì´í„°ë¥¼ analyzer í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                converted_data = {
                    'product_id': f"CPG_{i+1:06d}",  # CPG_000001 í˜•ì‹
                    'product_title': product.get('name', product.get('detail_name', 'N/A')),
                    'product_category': 'ë¸”ë£¨íˆ¬ìŠ¤ ì´ì–´í°',  # í˜„ì¬ëŠ” ê³ ì •ê°’, ë‚˜ì¤‘ì— íŒŒë¼ë¯¸í„°ë¡œ ë³€ê²½
                    'discounted_price': product.get('price', 0),
                    'product_rating': product.get('rating', 0.0),
                    'total_reviews': product.get('review_count', 0),
                    'purchased_last_month': max(1, product.get('review_count', 0) // 10),  # ë¦¬ë·°ìˆ˜ì˜ 10% ì¶”ì •
                    'brand': product.get('brand', 'N/A')[:50] if product.get('brand') != 'N/A' else 'N/A',  # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
                    'seller': product.get('seller', 'N/A')[:50] if product.get('seller') != 'N/A' else 'N/A',
                    'is_prime': 'Y' if product.get('is_prime', False) else 'N',
                    'scraped_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
                writer.writerow(converted_data)
        
        print(f"âœ… {len(products_data)}ê°œ ìƒí’ˆ ë°ì´í„°ë¥¼ {filename}ì— ì €ì¥ì™„ë£Œ!")
        return filepath

    def save_to_database(self, products_data, keyword: str):
        """
        ìˆ˜ì§‘í•œ ìƒí’ˆ ë°ì´í„°ë¥¼ SQLite ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤ (ORM ì‚¬ìš©).
        CSV ë°©ì‹ë³´ë‹¤ ì•ˆì „í•˜ê³  í™•ì¥ì„±ì´ ì¢‹ìŠµë‹ˆë‹¤.
        
        Args:
            products_data: ìƒí’ˆ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            keyword: ê²€ìƒ‰ì— ì‚¬ìš©ëœ í‚¤ì›Œë“œ
            
        Returns:
            dict: ì €ì¥ ê²°ê³¼ ì •ë³´
        """
        if not products_data:
            print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {"success": False, "message": "ë°ì´í„° ì—†ìŒ"}

        print(f"=== SQLite ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì‹œì‘ ===")
        
        # ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„± (ì—†ì„ ê²½ìš°)
        db_manager.create_tables()
        
        # ì„¸ì…˜ ì‹œì‘
        session = db_manager.get_session()
        session_start_time = datetime.utcnow()
        
        try:
            products_added = 0
            products_skipped = 0
            
            for i, product_data in enumerate(products_data):
                # ê³ ìœ  ìƒí’ˆ ID ìƒì„±
                product_id = f"CPG_{session_start_time.strftime('%Y%m%d_%H%M%S')}_{i+1:03d}"
                
                # ì¤‘ë³µ ì²´í¬ (URL ê¸°ì¤€)
                product_url = product_data.get('url', '')
                if product_url:
                    existing = session.query(Product).filter_by(product_url=product_url).first()
                    if existing:
                        products_skipped += 1
                        print(f"ì¤‘ë³µ ìƒí’ˆ ìŠ¤í‚µ: {product_data.get('name', 'N/A')[:30]}...")
                        continue
                
                # Product ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                product = Product(
                    product_id=product_id,
                    product_title=product_data.get('name', product_data.get('detail_name', 'N/A')),
                    product_category=keyword,  # ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì¹´í…Œê³ ë¦¬ë¡œ ì‚¬ìš©
                    discounted_price=product_data.get('price', 0),
                    product_rating=product_data.get('rating', 0.0),
                    total_reviews=product_data.get('review_count', 0),
                    purchased_last_month=max(1, product_data.get('review_count', 0) // 10),  # ë¦¬ë·°ìˆ˜ì˜ 10% ì¶”ì •
                    brand=product_data.get('brand') if product_data.get('brand') not in ['N/A', None] else None,
                    seller=product_data.get('seller') if product_data.get('seller') not in ['N/A', None] else None,
                    is_prime=product_data.get('is_prime', False),
                    product_url=product_url,
                    scraped_at=datetime.utcnow()
                )
                
                session.add(product)
                products_added += 1
                print(f"âœ… ìƒí’ˆ ì¶”ê°€: {product.product_title[:30]}... (ID: {product_id})")
            
            # ìŠ¤í¬ë ˆì´í•‘ ì„¸ì…˜ ì •ë³´ ì €ì¥
            scraping_session = ScrapingSession(
                keyword=keyword,
                products_found=len(products_data),
                products_saved=products_added,
                session_status='completed' if products_added > 0 else 'partial',
                error_message=f"{products_skipped}ê°œ ì¤‘ë³µ ìƒí’ˆ ìŠ¤í‚µë¨" if products_skipped > 0 else None,
                started_at=session_start_time,
                completed_at=datetime.utcnow()
            )
            session.add(scraping_session)
            
            # ì»¤ë°‹ (íŠ¸ëœì­ì…˜ ì™„ë£Œ)
            session.commit()
            
            result = {
                "success": True,
                "products_added": products_added,
                "products_skipped": products_skipped,
                "total_products_in_db": session.query(Product).count(),
                "message": f"ì„±ê³µì ìœ¼ë¡œ {products_added}ê°œ ìƒí’ˆì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤."
            }
            
            print(f"ğŸ‰ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ!")
            print(f"   - ìƒˆë¡œ ì¶”ê°€: {products_added}ê°œ")
            print(f"   - ì¤‘ë³µ ìŠ¤í‚µ: {products_skipped}ê°œ") 
            print(f"   - DB ì´ ìƒí’ˆ ìˆ˜: {result['total_products_in_db']}ê°œ")
            
            return result
            
        except Exception as e:
            session.rollback()
            error_message = f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            print(f"âŒ {error_message}")
            
            # ì‹¤íŒ¨í•œ ìŠ¤í¬ë ˆì´í•‘ ì„¸ì…˜ë„ ê¸°ë¡
            try:
                failed_session = ScrapingSession(
                    keyword=keyword,
                    products_found=len(products_data),
                    products_saved=0,
                    session_status='failed',
                    error_message=str(e),
                    started_at=session_start_time,
                    completed_at=datetime.utcnow()
                )
                session.add(failed_session)
                session.commit()
            except:
                pass  # ì„¸ì…˜ ì €ì¥ë„ ì‹¤íŒ¨í•˜ë©´ ë¬´ì‹œ
            
            return {"success": False, "message": error_message}
            
        finally:
            db_manager.close_session(session)

    async def scrape_and_save_to_db(self, keyword: str, max_products: int = 50):
        """
        í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•˜ì—¬ ìƒí’ˆ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  SQLite ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” í†µí•© ë©”ì„œë“œ (ORM ë²„ì „)
        """
        print(f"=== '{keyword}' í‚¤ì›Œë“œë¡œ ë°ì´í„° ìˆ˜ì§‘ ë° DB ì €ì¥ ì‹œì‘ ===")
        
        # 1ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì—ì„œ ê¸°ë³¸ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘
        scrape_result = await self.scrape_search_page(keyword)
        
        if scrape_result["status"] == "error":
            print(f"ê¸°ë³¸ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {scrape_result['reason']}")
            return {"success": False, "message": scrape_result["reason"]}
            
        products_basic = scrape_result["data"]
        # ìƒí’ˆ ìˆ˜ ì œí•œ
        products_basic = products_basic[:max_products]
        print(f"{len(products_basic)}ê°œ ìƒí’ˆì— ëŒ€í•´ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ì„ ì§„í–‰í•©ë‹ˆë‹¤...")
        
        # 2ë‹¨ê³„: ê° ìƒí’ˆì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (ì²˜ìŒ 5ê°œë§Œ í…ŒìŠ¤íŠ¸)
        detailed_products = []
        for i, product in enumerate(products_basic[:5]):  # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 5ê°œë§Œ
            print(f"[{i+1}/{min(5, len(products_basic))}] ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
            detail_info = await self.scrape_product_detail(product['url'])
            
            # ê¸°ë³¸ ì •ë³´ì™€ ìƒì„¸ ì •ë³´ ë³‘í•©
            merged_product = {**product, **detail_info}
            detailed_products.append(merged_product)
            
            # ìš”ì²­ ê°„ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            await asyncio.sleep(1)
        
        # 3ë‹¨ê³„: SQLite ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ORM ì‚¬ìš©)
        db_result = self.save_to_database(detailed_products, keyword)
        
        print(f"=== ë°ì´í„° ìˆ˜ì§‘ ë° DB ì €ì¥ ì™„ë£Œ! ===")
        print(f"ì €ì¥ ê²°ê³¼: {db_result.get('message', 'Unknown')}")
        
        return db_result

    async def scrape_and_save(self, keyword: str, max_products: int = 50):
        """
        í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•˜ì—¬ ìƒí’ˆ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  CSVë¡œ ì €ì¥í•˜ëŠ” í†µí•© ë©”ì„œë“œ
        """
        print(f"=== '{keyword}' í‚¤ì›Œë“œë¡œ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ ì‹œì‘ ===")
        
        # 1ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì—ì„œ ê¸°ë³¸ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘
        products_basic = await self.scrape_search_page(keyword)
        
        if not products_basic:
            print("ê¸°ë³¸ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None
            
        # ìƒí’ˆ ìˆ˜ ì œí•œ
        products_basic = products_basic[:max_products]
        print(f"{len(products_basic)}ê°œ ìƒí’ˆì— ëŒ€í•´ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ì„ ì§„í–‰í•©ë‹ˆë‹¤...")
        
        # 2ë‹¨ê³„: ê° ìƒí’ˆì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (ì²˜ìŒ 5ê°œë§Œ í…ŒìŠ¤íŠ¸)
        detailed_products = []
        for i, product in enumerate(products_basic[:5]):  # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 5ê°œë§Œ
            print(f"[{i+1}/{min(5, len(products_basic))}] ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
            detail_info = await self.scrape_product_detail(product['url'])
            
            # ê¸°ë³¸ ì •ë³´ì™€ ìƒì„¸ ì •ë³´ ë³‘í•©
            merged_product = {**product, **detail_info}
            detailed_products.append(merged_product)
            
            # ìš”ì²­ ê°„ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            await asyncio.sleep(1)
        
        # 3ë‹¨ê³„: CSV íŒŒì¼ë¡œ ì €ì¥
        csv_filepath = self.save_to_csv(detailed_products, f"{keyword.replace(' ', '_')}_products.csv")
        
        print(f"=== ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ ì™„ë£Œ! ===")
        print(f"ì €ì¥ëœ íŒŒì¼: {csv_filepath}")
        return csv_filepath

async def main():
    """CoupangScraperì˜ ìƒˆë¡œìš´ ORM ê¸°ë°˜ í†µí•© ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤: ë°ì´í„° ìˆ˜ì§‘ + SQLite ì €ì¥"""
    scraper = CoupangScraper()
    await scraper.start_browser()
    try:
        # ìƒˆë¡œìš´ ORM ê¸°ë°˜ í†µí•© ë©”ì„œë“œ ì‚¬ìš©: ê²€ìƒ‰ + ìƒì„¸ì •ë³´ ìˆ˜ì§‘ + SQLite ì €ì¥
        db_result = await scraper.scrape_and_save_to_db("ë¬´ì„ ë§ˆìš°ìŠ¤", max_products=20)
        
        if db_result and db_result.get('success'):
            print(f"\nğŸ‰ ì„±ê³µì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤!")
            print(f"ğŸ“Š ì €ì¥ëœ ìƒí’ˆ ìˆ˜: {db_result['products_added']}ê°œ")
            print(f"ğŸ“Š ì „ì²´ DB ìƒí’ˆ ìˆ˜: {db_result['total_products_in_db']}ê°œ")
        else:
            print(f"\nâŒ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            print(f"ì˜¤ë¥˜ ë©”ì‹œì§€: {db_result.get('message', 'Unknown error') if db_result else 'No result'}")

    finally:
        await scraper.close_browser()

if __name__ == '__main__':
    asyncio.run(main())