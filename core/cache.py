"""
Redis ìºì‹œ ë§¤ë‹ˆì €
ë¶„ì„ ê²°ê³¼, ìŠ¤í¬ë˜í•‘ ìƒíƒœ, ì„¸ì…˜ ë°ì´í„°ë¥¼ ìºì‹±í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
"""

import json
import redis
from datetime import datetime, timedelta
from typing import Any, Optional, Dict, List
import logging
import hashlib

logger = logging.getLogger(__name__)

class CacheManager:
    """Redis ê¸°ë°˜ ìºì‹œ ë§¤ë‹ˆì €"""
    
    def __init__(self, host='localhost', port=6379, db=0, decode_responses=True):
        """
        Redis ì—°ê²° ì´ˆê¸°í™”
        
        Args:
            host: Redis ì„œë²„ í˜¸ìŠ¤íŠ¸
            port: Redis ì„œë²„ í¬íŠ¸ 
            db: ë°ì´í„°ë² ì´ìŠ¤ ë²ˆí˜¸
            decode_responses: ì‘ë‹µ ìë™ ë””ì½”ë”© ì—¬ë¶€
        """
        try:
            self.redis_client = redis.Redis(
                host=host, 
                port=port, 
                db=db, 
                decode_responses=decode_responses,
                socket_connect_timeout=5,
                socket_timeout=5
            )
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            self.redis_client.ping()
            logger.info("âœ… Redis connection established successfully")
        except redis.ConnectionError as e:
            logger.error(f"âŒ Failed to connect to Redis: {e}")
            raise
        except Exception as e:
            logger.error(f"âŒ Redis initialization error: {e}")
            raise

    def _generate_key(self, prefix: str, identifier: str) -> str:
        """
        ìºì‹œ í‚¤ ìƒì„± (í•´ì‹œ ê¸°ë°˜)
        
        Args:
            prefix: í‚¤ ì ‘ë‘ì‚¬ (ì˜ˆ: 'analysis', 'scraping_status')  
            identifier: ê³ ìœ  ì‹ë³„ì (ì˜ˆ: í‚¤ì›Œë“œ, ì„¸ì…˜ ID)
            
        Returns:
            ìƒì„±ëœ ìºì‹œ í‚¤
        """
        # í‚¤ì›Œë“œë¥¼ í•´ì‹œí™”í•˜ì—¬ ê¸´ í‚¤ì›Œë“œë„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        key_hash = hashlib.md5(identifier.encode()).hexdigest()[:8]
        return f"market_insights:{prefix}:{key_hash}:{identifier}"

    def set_analysis_result(self, keyword: str, result_data: Dict[str, Any], ttl_hours: int = 1) -> bool:
        """
        ë¶„ì„ ê²°ê³¼ ìºì‹œ ì €ì¥
        
        Args:
            keyword: ë¶„ì„ í‚¤ì›Œë“œ
            result_data: ë¶„ì„ ê²°ê³¼ ë°ì´í„° 
            ttl_hours: ìºì‹œ ìœ íš¨ ì‹œê°„ (ì‹œê°„)
            
        Returns:
            ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            key = self._generate_key("analysis", keyword)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            cache_data = {
                'keyword': keyword,
                'result': result_data,
                'cached_at': datetime.now().isoformat(),
                'cache_ttl_hours': ttl_hours
            }
            
            # JSON ì§ë ¬í™”í•˜ì—¬ ì €ì¥
            success = self.redis_client.setex(
                key, 
                timedelta(hours=ttl_hours), 
                json.dumps(cache_data, ensure_ascii=False)
            )
            
            if success:
                logger.info(f"ğŸ“¦ Analysis result cached for keyword: '{keyword}' (TTL: {ttl_hours}h)")
                return True
            else:
                logger.warning(f"âš ï¸ Failed to cache analysis result for: '{keyword}'")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Error caching analysis result for '{keyword}': {e}")
            return False

    def get_analysis_result(self, keyword: str) -> Optional[Dict[str, Any]]:
        """
        ë¶„ì„ ê²°ê³¼ ìºì‹œ ì¡°íšŒ
        
        Args:
            keyword: ë¶„ì„ í‚¤ì›Œë“œ
            
        Returns:
            ìºì‹œëœ ë¶„ì„ ê²°ê³¼ (ì—†ìœ¼ë©´ None)
        """
        try:
            key = self._generate_key("analysis", keyword)
            cached_data = self.redis_client.get(key)
            
            if cached_data:
                result = json.loads(cached_data)
                logger.info(f"ğŸ¯ Cache HIT for analysis: '{keyword}'")
                return result['result']
            else:
                logger.info(f"ğŸ” Cache MISS for analysis: '{keyword}'")
                return None
                
        except Exception as e:
            logger.error(f"âŒ Error retrieving cached analysis for '{keyword}': {e}")
            return None

    def set_scraping_status(self, session_id: str, status_data: Dict[str, Any], ttl_minutes: int = 30) -> bool:
        """
        ìŠ¤í¬ë˜í•‘ ì§„í–‰ ìƒíƒœ ìºì‹œ ì €ì¥
        
        Args:
            session_id: ìŠ¤í¬ë˜í•‘ ì„¸ì…˜ ID
            status_data: ì§„í–‰ ìƒíƒœ ë°ì´í„°
            ttl_minutes: ìºì‹œ ìœ íš¨ ì‹œê°„ (ë¶„)
            
        Returns:
            ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            key = self._generate_key("scraping_status", session_id)
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
            status_data['updated_at'] = datetime.now().isoformat()
            
            success = self.redis_client.setex(
                key, 
                timedelta(minutes=ttl_minutes), 
                json.dumps(status_data, ensure_ascii=False)
            )
            
            if success:
                logger.debug(f"ğŸ“Š Scraping status updated for session: {session_id}")
                return True
            else:
                logger.warning(f"âš ï¸ Failed to update scraping status for: {session_id}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Error updating scraping status for '{session_id}': {e}")
            return False

    def get_scraping_status(self, session_id: str) -> Optional[Dict[str, Any]]:
        """
        ìŠ¤í¬ë˜í•‘ ì§„í–‰ ìƒíƒœ ì¡°íšŒ
        
        Args:
            session_id: ìŠ¤í¬ë˜í•‘ ì„¸ì…˜ ID
            
        Returns:
            ì§„í–‰ ìƒíƒœ ë°ì´í„° (ì—†ìœ¼ë©´ None)
        """
        try:
            key = self._generate_key("scraping_status", session_id)
            status_data = self.redis_client.get(key)
            
            if status_data:
                return json.loads(status_data)
            return None
                
        except Exception as e:
            logger.error(f"âŒ Error retrieving scraping status for '{session_id}': {e}")
            return None

    def delete_scraping_status(self, session_id: str) -> bool:
        """
        ìŠ¤í¬ë˜í•‘ ìƒíƒœ ìºì‹œ ì‚­ì œ (ì™„ë£Œ í›„ ì •ë¦¬)
        
        Args:
            session_id: ìŠ¤í¬ë˜í•‘ ì„¸ì…˜ ID
            
        Returns:
            ì‚­ì œ ì„±ê³µ ì—¬ë¶€  
        """
        try:
            key = self._generate_key("scraping_status", session_id)
            deleted = self.redis_client.delete(key)
            if deleted:
                logger.debug(f"ğŸ—‘ï¸ Scraping status deleted for session: {session_id}")
            return bool(deleted)
                
        except Exception as e:
            logger.error(f"âŒ Error deleting scraping status for '{session_id}': {e}")
            return False

    def get_cache_stats(self) -> Dict[str, Any]:
        """
        ìºì‹œ í†µê³„ ì •ë³´ ì¡°íšŒ
        
        Returns:
            ìºì‹œ í†µê³„ ë°ì´í„°
        """
        try:
            info = self.redis_client.info()
            stats = {
                'redis_version': info.get('redis_version', 'Unknown'),
                'connected_clients': info.get('connected_clients', 0),
                'used_memory_human': info.get('used_memory_human', '0B'),
                'used_memory_peak_human': info.get('used_memory_peak_human', '0B'),
                'total_commands_processed': info.get('total_commands_processed', 0),
                'keyspace_hits': info.get('keyspace_hits', 0),
                'keyspace_misses': info.get('keyspace_misses', 0),
            }
            
            # íˆíŠ¸ìœ¨ ê³„ì‚°
            hits = stats['keyspace_hits']
            misses = stats['keyspace_misses']
            total_requests = hits + misses
            
            if total_requests > 0:
                stats['cache_hit_rate'] = round((hits / total_requests) * 100, 2)
            else:
                stats['cache_hit_rate'] = 0.0
                
            return stats
            
        except Exception as e:
            logger.error(f"âŒ Error retrieving cache stats: {e}")
            return {}

    def flush_analysis_cache(self) -> int:
        """
        ë¶„ì„ ê²°ê³¼ ìºì‹œë§Œ ì „ì²´ ì‚­ì œ
        
        Returns:
            ì‚­ì œëœ í‚¤ ê°œìˆ˜
        """
        try:
            pattern = "market_insights:analysis:*"
            keys = self.redis_client.keys(pattern)
            if keys:
                deleted = self.redis_client.delete(*keys)
                logger.info(f"ğŸ§¹ Flushed {deleted} analysis cache entries")
                return deleted
            return 0
            
        except Exception as e:
            logger.error(f"âŒ Error flushing analysis cache: {e}")
            return 0

    def health_check(self) -> Dict[str, Any]:
        """
        Redis ì—°ê²° ìƒíƒœ ë° ì„±ëŠ¥ ì²´í¬
        
        Returns:
            í—¬ìŠ¤ ì²´í¬ ê²°ê³¼
        """
        health_status = {
            'status': 'unhealthy',
            'redis_connected': False,
            'response_time_ms': 0,
            'error': None
        }
        
        try:
            start_time = datetime.now()
            
            # ping í…ŒìŠ¤íŠ¸
            pong = self.redis_client.ping()
            
            end_time = datetime.now()
            response_time = (end_time - start_time).total_seconds() * 1000
            
            if pong:
                health_status.update({
                    'status': 'healthy',
                    'redis_connected': True,
                    'response_time_ms': round(response_time, 2)
                })
                
        except Exception as e:
            health_status['error'] = str(e)
            logger.error(f"âŒ Redis health check failed: {e}")
            
        return health_status

# ì „ì—­ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_cache_instance = None

def get_cache_manager() -> CacheManager:
    """
    ìºì‹œ ë§¤ë‹ˆì € ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
    """
    global _cache_instance
    if _cache_instance is None:
        _cache_instance = CacheManager()
    return _cache_instance